{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c25255-c191-4009-8f63-e51c2961009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/va0831/Projects/FlowMatchingMnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/va0831/slr/end_slr/lib64/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98775c9c-8761-40ef-b899-68b93721746b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conditional_mnist_diffusion_flow.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- Configuración ---\n",
    "device = 'cuda:4' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 128\n",
    "timesteps = 10\n",
    "img_shape = (1, 28, 28)\n",
    "os.makedirs(\"outputs/diffusion/images\", exist_ok=True)\n",
    "\n",
    "# --- Dataset ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- Modelo Condicional Transformer Denoiser ---\n",
    "class TransformerDenoiser(nn.Module):\n",
    "    def __init__(self, num_classes=10, dim=128, depth=4, heads=4):\n",
    "        super().__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, dim)\n",
    "        self.t_embed = nn.Linear(1, dim)\n",
    "        self.input_proj = nn.Linear(28 * 28, dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads), num_layers=depth\n",
    "        )\n",
    "        self.output_proj = nn.Linear(dim, 28 * 28)\n",
    "\n",
    "    def forward(self, x, noise_level, labels):\n",
    "        B = x.size(0)\n",
    "        x_flat = x.view(B, -1)  # (B, 784)\n",
    "        x_embed = self.input_proj(x_flat)\n",
    "        print(\"noise_level:\",noise_level.shape)\n",
    "        print(\"labels:\",labels.shape)\n",
    "        label_embed = self.label_embedding(labels)      # (B, dim)\n",
    "        t_embed = self.t_embed(noise_level)  # (B, dim)\n",
    "        print(\"t_embed:\",t_embed.shape)\n",
    "        cond = label_embed + t_embed\n",
    "        cond = cond#.unsqueeze(0)  # (1, B, dim)\n",
    "\n",
    "        print(\"cond:\",cond.shape)\n",
    "        x_cond = x_embed + cond  # broadcasting (1, B, dim)\n",
    "        print(\"x_cond:\",x_cond.shape)\n",
    "        transformed = self.transformer(x_cond)  # (1, B, dim)\n",
    "        out = self.output_proj(transformed.squeeze(0))  # (B, 784)\n",
    "        return out.view(B, 1, 28, 28)\n",
    "\n",
    "# --- Entrenamiento ---\n",
    "model = TransformerDenoiser().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        t = random.randint(1, timesteps)\n",
    "        noise_level = torch.tensor([t / timesteps], device=device).view(-1, 1, 1, 1)\n",
    "        noisy_images = images + torch.randn_like(images) * noise_level\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(noisy_images, noise_level, labels)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# --- Generación ---\n",
    "model.eval()\n",
    "prompt = random.randint(0, 9)\n",
    "with torch.no_grad():\n",
    "    denoised_img = torch.randn((1, 1, 28, 28)).to(device)\n",
    "    label = torch.tensor([prompt], device=device)\n",
    "\n",
    "    fig, axes = plt.subplots(1, timesteps + 1, figsize=(15, 3))\n",
    "    axes[0].imshow(denoised_img.cpu().squeeze(), cmap='gray')\n",
    "    axes[0].set_title(\"Start (Noise)\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    for t in reversed(range(1, timesteps + 1)):\n",
    "        noise_level = torch.tensor([[[[t / timesteps]]]], device=device)\n",
    "        denoised_img = model(denoised_img, noise_level, label)\n",
    "        axes[timesteps - t + 1].imshow(denoised_img.cpu().detach().squeeze(), cmap='gray')\n",
    "        axes[timesteps - t + 1].set_title(f\"Step {timesteps - t + 1}\\nDigit {prompt}\")\n",
    "        axes[timesteps - t + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e63c289-c213-45f2-ad24-0d84284f7d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_level.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d6782-e46e-430a-9700-fff5fa8e030f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc558bb3-adea-43c9-a948-1e1035187d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad75af3-3b40-4e17-ba3e-e64854293090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a976dac-004c-4d7d-93ab-c73a88cdfabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3920c-9710-4c49-8725-1dd2cd8170cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f2ba7-86f3-4c44-8d56-5aa092dcbe48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a4293-c3d1-450d-954d-3decc6430ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927b129-05ca-4156-975d-ad20c4c5c20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd67550-00e1-4032-9ec5-bbeabeb1bc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c656e7-9f5f-418e-8c71-6c1f87884708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
